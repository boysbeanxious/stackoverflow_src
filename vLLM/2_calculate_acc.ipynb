{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "p = os.path.abspath('..')\n",
    "# p = p+r'\\config'\n",
    "sys.path.insert(1, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './result'\n",
    "file_list = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sc_calc_acc_condition_with_temp_with_sc(llm_model, few_shot_n, test_n, q_src_yn, ver, p_ver, sc_num, temp, excel_ver):\n",
    "    # ./result/sc_l_result_4_15_Y_30_sys_prompt8_0.01_ver1_0.csv\n",
    "    # print(f'sc_{llm_model}_result_{few_shot_n}_{test_n}_{q_src_yn}_{ver}_{p_ver}_{sc_num}_{temp}_{excel_ver}')\n",
    "    tmp = pd.DataFrame()\n",
    "    df_eval = pd.DataFrame()\n",
    "    acc_list = []\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    file_list = os.listdir(path)\n",
    "    opt_file = [x for x in file_list if x.startswith(f'sc_{llm_model}_result_{few_shot_n}_{test_n}_{q_src_yn}_{ver}_{p_ver}_{sc_num}_{temp}_{excel_ver}')]\n",
    "    opt_file = [x for x in opt_file if x.endswith(f'.csv')]\n",
    "\n",
    "    if len(opt_file)>0 : \n",
    "        print(f'완료된 파일 개수 : {len(opt_file)}')\n",
    "        for f in opt_file:\n",
    "            tmp = pd.read_csv(f'{path}/{f}', index_col =0)\n",
    "            tmp = tmp.dropna()\n",
    "\n",
    "            tmp['gold'] = tmp['answer_encode'].apply(lambda x : re.sub(r'[^012]', '', x))\n",
    "            tmp['o_result'] = tmp['result'].apply(lambda x : re.sub(r'[^012]', '', x))\n",
    "            tmp = tmp[tmp['o_result'].isin(['1', '0', '2'])]\n",
    "\n",
    "            \n",
    "            gold_df = tmp[['id', 'gold']].drop_duplicates()\n",
    "            chk_cnt = tmp.groupby(['id', 'o_result']).count().reset_index()[['id', 'o_result', 'question']]\n",
    "            chk_cnt = chk_cnt.rename(columns = {'question': 'cnt'})\n",
    "            chk_cnt = chk_cnt[chk_cnt['cnt'] == sc_num]\n",
    "            chk_cnt = chk_cnt.sort_values(by = ['id', 'cnt'], ascending=[True, False]).groupby(['id']).head(1)\n",
    "            df_eval = pd.merge(gold_df, chk_cnt, on = ['id'])\n",
    "\n",
    "            df_eval['equal_yn'] = np.where(df_eval['gold']==df_eval['o_result'], 1, 0)\n",
    "            acc = (df_eval['equal_yn'].sum()/df_eval.shape[0])*100  \n",
    "            acc_list.append(acc)\n",
    "            df = pd.concat([df, df_eval], axis =0)\n",
    "            \n",
    "        df['equal_yn'] = np.where(df['gold']==df['o_result'], 1, 0)\n",
    "        y_true = df['o_result']\n",
    "        y_pred = df['gold']\n",
    "        print(metrics.classification_report(y_true, y_pred, digits=3))\n",
    "\n",
    "        acc = (df['equal_yn'].sum()/df.shape[0])*100            \n",
    "        print(f'{llm_model}_result_{few_shot_n}_{test_n}_{q_src_yn} : ', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.889     0.941        18\n",
      "           1      0.875     1.000     0.933        14\n",
      "           2      1.000     1.000     1.000         2\n",
      "\n",
      "    accuracy                          0.941        34\n",
      "   macro avg      0.958     0.963     0.958        34\n",
      "weighted avg      0.949     0.941     0.941        34\n",
      "\n",
      "v_result_4_10_Y :  94.11764705882352\n"
     ]
    }
   ],
   "source": [
    "# process3 = Process(target=task, args=('v', 4, 10, 'Y', 10, 'sys_prompt10', 5, 0.01, 'ver5'))\n",
    "sc_calc_acc_condition_with_temp_with_sc('v', 4, 10, 'Y', 10, 'sys_prompt10', 5,  0.01, 'ver5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.868     0.781     0.822       320\n",
      "           1      0.720     0.862     0.785       298\n",
      "           2      0.938     0.659     0.774        91\n",
      "\n",
      "    accuracy                          0.800       709\n",
      "   macro avg      0.842     0.768     0.794       709\n",
      "weighted avg      0.815     0.800     0.800       709\n",
      "\n",
      "v_result_4_30_Y :  79.97179125528913\n"
     ]
    }
   ],
   "source": [
    "# process3 = Process(target=task, args=('v', 4, 10, 'Y', 10, 'sys_prompt10', 5, 0.01, 'ver5'))\n",
    "# ('v', 4, 30, 'Y', 100, 'sys_prompt10', 5, 0.01, 'ver5')\n",
    "sc_calc_acc_condition_with_temp_with_sc('v', 4, 30, 'Y', 100, 'sys_prompt10', 5,  0.01, 'ver5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_stackoverflow_src",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
