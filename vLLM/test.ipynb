{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f08474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79a0ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U blobfile tiktoken \"transformers>=4.44\" \"tokenizers>=0.19\" sentencepiece safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121447f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386b2b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !huggingface-cli download meta-llama/Llama-3.2-1B-Instruct --local-dir /usr/share/d_ollama/.ollama/models/hf_model --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea28dca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-03 20:59:36 [utils.py:328] non-default args: {'tensor_parallel_size': 4, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'model': '/usr/share/d_ollama/.ollama/models/hf_model/Llama-3.2-1B-Instruct'}\n",
      "INFO 10-03 20:59:36 [__init__.py:742] Resolved architecture: LlamaForCausalLM\n",
      "INFO 10-03 20:59:36 [__init__.py:1815] Using max model len 131072\n",
      "INFO 10-03 20:59:36 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m INFO 10-03 20:59:36 [core.py:654] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m INFO 10-03 20:59:36 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='/usr/share/d_ollama/.ollama/models/hf_model/Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='/usr/share/d_ollama/.ollama/models/hf_model/Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/usr/share/d_ollama/.ollama/models/hf_model/Llama-3.2-1B-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m WARNING 10-03 20:59:36 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m INFO 10-03 20:59:36 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_fe5ee569'), local_subscribe_addr='ipc:///tmp/0848f53a-fcb0-4ea6-b650-4fe54a890dec', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m INFO 10-03 20:59:39 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f09a5e37'), local_subscribe_addr='ipc:///tmp/9b2d0bee-3738-456f-8ae0-d24ccf25f3d1', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m INFO 10-03 20:59:39 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9bf94cbf'), local_subscribe_addr='ipc:///tmp/59c51e4d-9f32-4d0b-8e39-bf908952066d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m INFO 10-03 20:59:39 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_dd17a24b'), local_subscribe_addr='ipc:///tmp/576dbf9d-a980-4f61-9ed8-8456b9d4289d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 10-03 20:59:39 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_feaf4773'), local_subscribe_addr='ipc:///tmp/73d2bab8-8cea-450c-853e-72eedb49cba1', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:40 [multiproc_executor.py:585] WorkerProc failed to start.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:40 [multiproc_executor.py:585] Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:40 [multiproc_executor.py:585]   File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py\", line 559, in worker_main\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:40 [multiproc_executor.py:585]     worker = WorkerProc(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:40 [multiproc_executor.py:585]   File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py\", line 420, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:40 [multiproc_executor.py:585]     self.worker.init_device()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:40 [multiproc_executor.py:585]   File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/vllm/worker/worker_base.py\", line 611, in init_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:40 [multiproc_executor.py:585]     self.worker.init_device()  # type: ignore\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:40 [multiproc_executor.py:585]   File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\", line 180, in init_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:40 [multiproc_executor.py:585]     raise ValueError(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:40 [multiproc_executor.py:585] ValueError: Free memory on device (0.6/23.55 GiB) on startup is less than desired GPU memory utilization (0.3, 7.06 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fd09f8aba00>>\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m   File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m     def _clean_thread_parent_frames(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m   File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py\", line 526, in signal_handler\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m     raise SystemExit()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m SystemExit: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:40 [multiproc_executor.py:585] WorkerProc failed to start.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:40 [multiproc_executor.py:585] Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:40 [multiproc_executor.py:585]   File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py\", line 559, in worker_main\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:40 [multiproc_executor.py:585]     worker = WorkerProc(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:40 [multiproc_executor.py:585]   File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py\", line 420, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:40 [multiproc_executor.py:585]     self.worker.init_device()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:40 [multiproc_executor.py:585]   File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/vllm/worker/worker_base.py\", line 611, in init_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:40 [multiproc_executor.py:585]     self.worker.init_device()  # type: ignore\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:40 [multiproc_executor.py:585]   File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\", line 180, in init_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:40 [multiproc_executor.py:585]     raise ValueError(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:40 [multiproc_executor.py:585] ValueError: Free memory on device (0.6/23.55 GiB) on startup is less than desired GPU memory utilization (0.3, 7.06 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1003 20:59:40.170340115 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:44 [core.py:718] EngineCore failed to start.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:44 [core.py:718] Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:44 [core.py:718]   File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 709, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:44 [core.py:718]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:44 [core.py:718]   File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 505, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:44 [core.py:718]     super().__init__(vllm_config, executor_class, log_stats,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:44 [core.py:718]   File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 82, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:44 [core.py:718]     self.model_executor = executor_class(vllm_config)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:44 [core.py:718]   File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 54, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:44 [core.py:718]     self._init_executor()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:44 [core.py:718]   File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py\", line 99, in _init_executor\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:44 [core.py:718]     self.workers = WorkerProc.wait_for_ready(unready_workers)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:44 [core.py:718]   File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py\", line 497, in wait_for_ready\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:44 [core.py:718]     raise e from None\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m ERROR 10-03 20:59:44 [core.py:718] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m Process EngineCore_DP0:\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m   File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m     self.run()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m   File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m   File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 722, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m     raise e\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m   File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 709, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m   File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 505, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m     super().__init__(vllm_config, executor_class, log_stats,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m   File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 82, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m   File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 54, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m     self._init_executor()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m   File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py\", line 99, in _init_executor\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m   File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py\", line 497, in wait_for_ready\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m     raise e from None\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3940778)\u001b[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.\n",
      "/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 3 leaked shared_memory objects to clean up at shutdown\n",
      "  warnings.warn('resource_tracker: There appear to be %d '\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLM\n\u001b[0;32m----> 3\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/usr/share/d_ollama/.ollama/models/hf_model/Llama-3.2-1B-Instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# or 4, since you have 4 GPUs\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/vllm/entrypoints/llm.py:282\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, runner, convert, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, override_pooler_config, kv_cache_memory_bytes, compilation_config, logits_processors, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m log_non_default_args(engine_args)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine)\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/vllm/engine/llm_engine.py:493\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLMEngine \u001b[38;5;28;01mas\u001b[39;00m V1LLMEngine\n\u001b[1;32m    491\u001b[0m     engine_cls \u001b[38;5;241m=\u001b[39m V1LLMEngine\n\u001b[0;32m--> 493\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_vllm_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py:134\u001b[0m, in \u001b[0;36mLLMEngine.from_vllm_config\u001b[0;34m(cls, vllm_config, usage_context, stat_loggers, disable_log_stats)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_vllm_config\u001b[39m(\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m     disable_log_stats: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    133\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLMEngine\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m               \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mExecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m               \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m               \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m               \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m               \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVLLM_ENABLE_V1_MULTIPROCESSING\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py:111\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_processor\u001b[38;5;241m.\u001b[39mtracer \u001b[38;5;241m=\u001b[39m tracer\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_core \u001b[38;5;241m=\u001b[39m \u001b[43mEngineCoreClient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m multiprocess_mode:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# for v0 compatibility\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_core\u001b[38;5;241m.\u001b[39mengine_core\u001b[38;5;241m.\u001b[39mmodel_executor  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/vllm/v1/engine/core_client.py:80\u001b[0m, in \u001b[0;36mEngineCoreClient.make_client\u001b[0;34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m EngineCoreClient\u001b[38;5;241m.\u001b[39mmake_async_mp_client(\n\u001b[1;32m     77\u001b[0m         vllm_config, executor_class, log_stats)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multiprocess_mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio_mode:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSyncMPClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m InprocClient(vllm_config, executor_class, log_stats)\n",
      "File \u001b[0;32m~/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/vllm/v1/engine/core_client.py:602\u001b[0m, in \u001b[0;36mSyncMPClient.__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vllm_config: VllmConfig, executor_class: \u001b[38;5;28mtype\u001b[39m[Executor],\n\u001b[1;32m    601\u001b[0m              log_stats: \u001b[38;5;28mbool\u001b[39m):\n\u001b[0;32m--> 602\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_dp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvllm_config\u001b[38;5;241m.\u001b[39mparallel_config\u001b[38;5;241m.\u001b[39mdata_parallel_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs_queue \u001b[38;5;241m=\u001b[39m queue\u001b[38;5;241m.\u001b[39mQueue[Union[EngineCoreOutputs, \u001b[38;5;167;01mException\u001b[39;00m]]()\n",
      "File \u001b[0;32m~/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/vllm/v1/engine/core_client.py:448\u001b[0m, in \u001b[0;36mMPClient.__init__\u001b[0;34m(self, asyncio_mode, vllm_config, executor_class, log_stats, client_addresses)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats_update_address \u001b[38;5;241m=\u001b[39m client_addresses\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstats_update_address\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;66;03m# Engines are managed by this client.\u001b[39;00m\n\u001b[0;32m--> 448\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m launch_core_engines(vllm_config, executor_class,\n\u001b[1;32m    449\u001b[0m                              log_stats) \u001b[38;5;28;01mas\u001b[39;00m (engine_manager,\n\u001b[1;32m    450\u001b[0m                                             coordinator,\n\u001b[1;32m    451\u001b[0m                                             addresses):\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresources\u001b[38;5;241m.\u001b[39mcoordinator \u001b[38;5;241m=\u001b[39m coordinator\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresources\u001b[38;5;241m.\u001b[39mengine_manager \u001b[38;5;241m=\u001b[39m engine_manager\n",
      "File \u001b[0;32m/usr/lib/python3.10/contextlib.py:142\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m         \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/vllm/v1/engine/utils.py:729\u001b[0m, in \u001b[0;36mlaunch_core_engines\u001b[0;34m(vllm_config, executor_class, log_stats, num_api_servers)\u001b[0m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m local_engine_manager, coordinator, addresses\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# Now wait for engines to start.\u001b[39;00m\n\u001b[0;32m--> 729\u001b[0m \u001b[43mwait_for_engine_startup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhandshake_socket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43maddresses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengines_to_handshake\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_engine_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/vllm/v1/engine/utils.py:782\u001b[0m, in \u001b[0;36mwait_for_engine_startup\u001b[0;34m(handshake_socket, addresses, core_engines, parallel_config, cache_config, proc_manager, coord_process)\u001b[0m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m coord_process \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m coord_process\u001b[38;5;241m.\u001b[39mexitcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    781\u001b[0m         finished[coord_process\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m coord_process\u001b[38;5;241m.\u001b[39mexitcode\n\u001b[0;32m--> 782\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine core initialization failed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    783\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee root cause above. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    784\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed core proc(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinished\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Receive HELLO and READY messages from the input socket.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m eng_identity, ready_msg_bytes \u001b[38;5;241m=\u001b[39m handshake_socket\u001b[38;5;241m.\u001b[39mrecv_multipart()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"/usr/share/d_ollama/.ollama/models/hf_model/Llama-3.2-1B-Instruct\",\n",
    "    tensor_parallel_size=4,   # or 4, since you have 4 GPUs\n",
    "    dtype=\"auto\",\n",
    "    gpu_memory_utilization=0.3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05bd214",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_stackoverflow_src",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
