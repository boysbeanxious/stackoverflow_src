{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://wikidocs.net/233348"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain_teddynote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import langchain\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_teddynote.messages import stream_response\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "# https://wikidocs.net/233348"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/241113_llama_ver1.csv', encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"llama-3.1-70b-instruct-lorablated.Q4_K_M:latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = np.arange(44)\n",
    "np.random.seed(1111)\n",
    "first_ann_q_id = np.random.choice(population, size=4, replace=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6, 23, 41, 25])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_ann_q_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is the question about software development. The title of the question is \"Why is this gunicorn, flask, bokeh application not running with 4 workers?\". and body of the question is \"<p>I am learning to use gunicorn, flask and bokeh.\\nI am trying to set up a little example to use a bokeh app with multiuser. I am working on a debian system on an AWS server with a static IP. I used the <a href=\"https://github.com/bokeh/bokeh/blob/2.4.3/examples/howto/server_embed/flask_gunicorn_embed.py\" rel=\"nofollow noreferrer\">example from git</a> and edited it in the way I think it should work. I got it to run with this command:</p>\\n<pre><code>gunicorn3 -w 1 --bind 0.0.0.0:8000 wsgi:app\\n</code></pre>\\n<p>But when I change the workers to 4 and the binded port from 5006 to 0 (flaskapp.py), I get this error in the browser console:</p>\\n<blockquote>\\n<p>:34163/bkapp/autoload.js?bokeh-autoload-element=4755&amp;bokeh-app-path=/bkapp&amp;bokeh-absolute-url=http://35.XXX.XXX.153:34163/bkapp:1          Failed to load resource: net::ERR_CONNECTION_TIMED_OUT</p>\\n</blockquote>\\n<p>Can someone tell me, why I get this error?</p>\\n<p>Here is the relevant code:</p>\\n<p><strong>flaskapp.py</strong></p>\\n<pre><code>\\nimport asyncio\\nfrom threading import Thread\\n\\nfrom flask import Flask, render_template\\nfrom tornado.httpserver import HTTPServer\\nfrom tornado.ioloop import IOLoop\\n\\nfrom bokeh.application import Application\\nfrom bokeh.application.handlers import FunctionHandler\\nfrom bokeh.embed import server_document\\nfrom bokeh.layouts import column\\nfrom bokeh.models import ColumnDataSource, Slider\\nfrom bokeh.plotting import figure\\nfrom bokeh.sampledata.sea_surface_temperature import sea_surface_temperature\\nfrom bokeh.server.server import BaseServer\\nfrom bokeh.server.tornado import BokehTornado\\nfrom bokeh.server.util import bind_sockets\\nfrom bokeh.themes import Theme\\n\\napp = Flask(__name__)\\n\\ndef bkapp(doc):\\ndf = sea_surface_temperature.copy()\\nsource = ColumnDataSource(data=df)\\n\\n    plot = figure(x_axis_type=\\'datetime\\', y_range=(0, 25), y_axis_label=\\'Temperature (Celsius)\\',\\n                  title=&quot;Sea Surface Temperature at 43.18, -70.43&quot;)\\n    plot.line(\\'time\\', \\'temperature\\', source=source)\\n    \\n    def callback(attr, old, new):\\n        if new == 0:\\n            data = df\\n        else:\\n            data = df.rolling(f&quot;{new}D&quot;).mean()\\n        source.data = ColumnDataSource.from_df(data)\\n    \\n    slider = Slider(start=0, end=30, value=0, step=1, title=&quot;Smoothing by N Days&quot;)\\n    slider.on_change(\\'value\\', callback)\\n    \\n    doc.add_root(column(slider, plot))\\n    \\n    doc.theme = Theme(filename=&quot;theme.yaml&quot;)\\n\\n# can\\'t use shortcuts here, since we are passing to low level BokehTornado\\n\\nbkapp = Application(FunctionHandler(bkapp))\\n\\n# This is so that if this app is run using something like &quot;gunicorn -w 4&quot; then\\n\\n# each process will listen on its own port\\n\\nsockets, port = bind_sockets(&quot;0.0.0.0&quot;, 0)\\n\\n@app.route(\\'/\\', methods=\\\\[\\'GET\\'\\\\])\\ndef bkapp_page():\\nscript = server_document(\\'http://35.XXX.XXX.153:%d/bkapp\\' % port)\\nprint(&quot;app port: %s&quot;, port)\\nreturn render_template(&quot;embed.html&quot;, script=script, template=&quot;Flask&quot;)\\n\\ndef bk_worker():\\nasyncio.set_event_loop(asyncio.new_event_loop())\\n\\n    bokeh_tornado = BokehTornado({\\'/bkapp\\': bkapp}, extra_websocket_origins=[&quot;*&quot;])\\n    bokeh_http = HTTPServer(bokeh_tornado)\\n    bokeh_http.add_sockets(sockets)\\n    \\n    server = BaseServer(IOLoop.current(), bokeh_tornado, bokeh_http)\\n    server.start()\\n    server.io_loop.start()\\n\\nt = Thread(target=bk_worker)\\nt.daemon = True\\nt.start()\\n</code></pre>\\n<p><strong>wsgi.py</strong></p>\\n<pre><code>\\nfrom flaskapp import app\\n\\nif __name__ == &quot;__main__&quot;:\\napp.run()\\n</code></pre>\\n<p><strong>embed.html</strong></p>\\n<pre><code>\\n\\\\&lt;!DOCTYPE html\\\\&gt;\\n\\\\&lt;html lang=&quot;en&quot;\\\\&gt;\\n\\\\&lt;head\\\\&gt;\\n\\\\&lt;meta charset=&quot;utf-8&quot;\\\\&gt;\\n{{ bokeh_css }}\\n{{ bokeh_js }}\\n\\\\&lt;link rel=&quot;stylesheet&quot; href=&quot;./static/css/styles.css&quot; type=&quot;text/css&quot;/\\\\&gt;\\n\\\\&lt;link rel=&quot;shortcut icon&quot; href=&quot;./static/favicon/favicon.ico&quot; type=&quot;image/x-icon&quot;\\\\&gt;\\n\\\\&lt;/head\\\\&gt;\\n\\\\&lt;body\\\\&gt;\\n\\\\&lt;div class=&quot;head&quot;\\\\&gt;\\n\\\\&lt;a href=&quot;http://&quot;\\\\&gt;\\\\&lt;img src=&quot;./static/images/logo.png&quot; width=&quot;93px&quot; height=&quot;36px&quot;/\\\\&gt;\\\\&lt;/a\\\\&gt;\\n\\\\&lt;/div\\\\&gt;\\n\\\\&lt;div class=&quot;body&quot;\\\\&gt;\\n{{ script|safe }}\\n\\\\&lt;/div\\\\&gt;\\n\\\\&lt;/body\\\\&gt;\\n\\\\&lt;/html\\\\&gt;\\n</code></pre>\\n\". Please explain about the \\'difficulty class\\' and \\'general ruleset\\' and \\'grenular breakdown\\' for question'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "for idx in first_ann_q_id[:3]:\n",
    "    temp_dict = {\"question\" : df.iloc[idx, 0],\n",
    "                 \"answer\"   : df.iloc[idx, 1]}\n",
    "    examples.append(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Here is the question about software development. The title of the question is \"Strange error: TypeError: forward() takes 2 positional arguments but 3 were given\". and body of the question is \"<p>I am trying to train a VAE model. However I keep getting the following error:</p>\\n<pre><code>Traceback (most recent call last):\\n  File &quot;/Users/devcharaf/Documents/Uni/UdeM/Internship/newGNN/app/train.py&quot;, line 28, in &lt;module&gt;\\n    trained_model, loss = train(model, train_data, optimizer, num_epochs=1000, model_type=&quot;VAE&quot;)\\n  File &quot;/Users/devcharaf/Documents/Uni/UdeM/Internship/newGNN/app/utils.py&quot;, line 322, in train\\n    recon_x, mean, logvar = model(data[&quot;x&quot;], data[&quot;edge_index&quot;])\\n  File &quot;/opt/anaconda3/envs/pygeometric/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1194, in _call_impl\\n    return forward_call(*input, **kwargs)\\n  File &quot;/Users/devcharaf/Documents/Uni/UdeM/Internship/newGNN/app/model.py&quot;, line 47, in forward\\n    mean, logvar = self.encoder_forward(x, edge_index)\\n  File &quot;/Users/devcharaf/Documents/Uni/UdeM/Internship/newGNN/app/model.py&quot;, line 53, in encoder_forward\\n    x = self.encoder(x, edge_index)\\n  File &quot;/opt/anaconda3/envs/pygeometric/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1194, in _call_impl\\n    return forward_call(*input, **kwargs)\\nTypeError: forward() takes 2 positional arguments but 3 were given\\n</code></pre>\\n<p>I tried many combinations of arguments but nothing seems to work. Can you spot the error?\\nAs you can see in the following model, the forward function was called in the train() function with only two arguments and not 3. Also the error hints at the line self.encoder(x, edge_index). But when I try to remove one of these arguments, I get an error saying not enough arguments.\\nHere is my model:</p>\\n<pre><code># Variational Auto Encoder\\nclass VAE(nn.Module):\\n    def __init__(self, in_dim, hidden_dim, latent_dim):\\n        super(VAE, self).__init__()\\n        self.in_dim = in_dim\\n        self.hidden_dim = hidden_dim\\n        self.latent_dim = latent_dim\\n\\n        self.encoder = nn.Sequential(\\n            gnn.GCNConv(in_dim, hidden_dim),\\n            nn.ReLU(),\\n            gnn.GCNConv(hidden_dim, hidden_dim),\\n            nn.ReLU()\\n        )\\n        self.fc_mean = nn.Linear(hidden_dim, latent_dim)\\n        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\\n        self.decoder = nn.Sequential(\\n            gnn.GCNConv(latent_dim, hidden_dim),\\n            nn.ReLU(),\\n            gnn.GCNConv(hidden_dim, in_dim),\\n            nn.Sigmoid()\\n        )\\n\\n    def forward(self, x, edge_index):\\n        mean, logvar = self.encoder_forward(x, edge_index)\\n        z = self.reparameterize(mean, logvar)\\n        recon_x = self.decoder_forward(z, edge_index)\\n        return recon_x, mean, logvar\\n\\n    def encoder_forward(self, x, edge_index):\\n        x = self.encoder(x, edge_index)\\n        mean = self.fc_mean(x)\\n        logvar = self.fc_logvar(x)\\n        return mean, logvar\\n\\n    def decoder_forward(self, x, edge_index):\\n        x = self.decoder(x, edge_index)\\n        return x\\n\\n    def reparameterize(self, mean, logvar):\\n        std = torch.exp(0.5 * logvar)\\n        eps = torch.randn_like(std)\\n        return eps.mul(std).add_(mean)\\n</code></pre>\\n<p>and here is my train function:</p>\\n<pre><code>def train(model, data, optimizer, num_epochs):\\n    model.train()\\n    criterion = nn.MSELoss()\\n    beta = 1\\n    epoch_losses = []\\n    for epoch in range(num_epochs):\\n        optimizer.zero_grad()\\n\\n        # Output of the model\\n        recon_x, mean, logvar = model(data[&quot;x&quot;], data[&quot;edge_index&quot;])\\n        output = recon_x\\n        kl_loss = -0.5 * torch.mean(1 + logvar - mean.pow(2) - logvar.exp())\\n        kl_loss *= beta\\n        kl_loss.backward(retain_graph=True)\\n\\n        target = torch.zeros(data[&quot;num_nodes&quot;], 1)\\n\\n        # Loss computation\\n        loss = criterion(output, target)\\n        loss += kl_loss\\n\\n        loss.backward()\\n        optimizer.step()\\n        epoch_losses.append(loss.item())\\n    \\n    return model, epoch_losses\\n</code></pre>\\n<p>Here is the code I am using to train the model:</p>\\n<pre><code>model = VAE(in_dim=10, hidden_dim=32, latent_dim=8)\\n\\n# Create an instance of the optimizer.\\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\\n\\n# Train the model for a specified number of epochs.\\ntrained_model, loss = train(model, train_data, optimizer, num_epochs=1000)\\n</code></pre>\\n\". Please explain about the \\'difficulty class\\' and \\'general ruleset\\' and \\'grenular breakdown\\' for question',\n",
       " 'answer': 'difficulty class : Basic Level Knowledge,general ruleset : B1-Syntax,granular breakdown : 기본 문법/타입 변환'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "Here is the question about software development. The title of the question is \"Strange error: TypeError: forward() takes 2 positional arguments but 3 were given\". and body of the question is \"<p>I am trying to train a VAE model. However I keep getting the following error:</p>\n",
      "<pre><code>Traceback (most recent call last):\n",
      "  File &quot;/Users/devcharaf/Documents/Uni/UdeM/Internship/newGNN/app/train.py&quot;, line 28, in &lt;module&gt;\n",
      "    trained_model, loss = train(model, train_data, optimizer, num_epochs=1000, model_type=&quot;VAE&quot;)\n",
      "  File &quot;/Users/devcharaf/Documents/Uni/UdeM/Internship/newGNN/app/utils.py&quot;, line 322, in train\n",
      "    recon_x, mean, logvar = model(data[&quot;x&quot;], data[&quot;edge_index&quot;])\n",
      "  File &quot;/opt/anaconda3/envs/pygeometric/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File &quot;/Users/devcharaf/Documents/Uni/UdeM/Internship/newGNN/app/model.py&quot;, line 47, in forward\n",
      "    mean, logvar = self.encoder_forward(x, edge_index)\n",
      "  File &quot;/Users/devcharaf/Documents/Uni/UdeM/Internship/newGNN/app/model.py&quot;, line 53, in encoder_forward\n",
      "    x = self.encoder(x, edge_index)\n",
      "  File &quot;/opt/anaconda3/envs/pygeometric/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "TypeError: forward() takes 2 positional arguments but 3 were given\n",
      "</code></pre>\n",
      "<p>I tried many combinations of arguments but nothing seems to work. Can you spot the error?\n",
      "As you can see in the following model, the forward function was called in the train() function with only two arguments and not 3. Also the error hints at the line self.encoder(x, edge_index). But when I try to remove one of these arguments, I get an error saying not enough arguments.\n",
      "Here is my model:</p>\n",
      "<pre><code># Variational Auto Encoder\n",
      "class VAE(nn.Module):\n",
      "    def __init__(self, in_dim, hidden_dim, latent_dim):\n",
      "        super(VAE, self).__init__()\n",
      "        self.in_dim = in_dim\n",
      "        self.hidden_dim = hidden_dim\n",
      "        self.latent_dim = latent_dim\n",
      "\n",
      "        self.encoder = nn.Sequential(\n",
      "            gnn.GCNConv(in_dim, hidden_dim),\n",
      "            nn.ReLU(),\n",
      "            gnn.GCNConv(hidden_dim, hidden_dim),\n",
      "            nn.ReLU()\n",
      "        )\n",
      "        self.fc_mean = nn.Linear(hidden_dim, latent_dim)\n",
      "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
      "        self.decoder = nn.Sequential(\n",
      "            gnn.GCNConv(latent_dim, hidden_dim),\n",
      "            nn.ReLU(),\n",
      "            gnn.GCNConv(hidden_dim, in_dim),\n",
      "            nn.Sigmoid()\n",
      "        )\n",
      "\n",
      "    def forward(self, x, edge_index):\n",
      "        mean, logvar = self.encoder_forward(x, edge_index)\n",
      "        z = self.reparameterize(mean, logvar)\n",
      "        recon_x = self.decoder_forward(z, edge_index)\n",
      "        return recon_x, mean, logvar\n",
      "\n",
      "    def encoder_forward(self, x, edge_index):\n",
      "        x = self.encoder(x, edge_index)\n",
      "        mean = self.fc_mean(x)\n",
      "        logvar = self.fc_logvar(x)\n",
      "        return mean, logvar\n",
      "\n",
      "    def decoder_forward(self, x, edge_index):\n",
      "        x = self.decoder(x, edge_index)\n",
      "        return x\n",
      "\n",
      "    def reparameterize(self, mean, logvar):\n",
      "        std = torch.exp(0.5 * logvar)\n",
      "        eps = torch.randn_like(std)\n",
      "        return eps.mul(std).add_(mean)\n",
      "</code></pre>\n",
      "<p>and here is my train function:</p>\n",
      "<pre><code>def train(model, data, optimizer, num_epochs):\n",
      "    model.train()\n",
      "    criterion = nn.MSELoss()\n",
      "    beta = 1\n",
      "    epoch_losses = []\n",
      "    for epoch in range(num_epochs):\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # Output of the model\n",
      "        recon_x, mean, logvar = model(data[&quot;x&quot;], data[&quot;edge_index&quot;])\n",
      "        output = recon_x\n",
      "        kl_loss = -0.5 * torch.mean(1 + logvar - mean.pow(2) - logvar.exp())\n",
      "        kl_loss *= beta\n",
      "        kl_loss.backward(retain_graph=True)\n",
      "\n",
      "        target = torch.zeros(data[&quot;num_nodes&quot;], 1)\n",
      "\n",
      "        # Loss computation\n",
      "        loss = criterion(output, target)\n",
      "        loss += kl_loss\n",
      "\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        epoch_losses.append(loss.item())\n",
      "    \n",
      "    return model, epoch_losses\n",
      "</code></pre>\n",
      "<p>Here is the code I am using to train the model:</p>\n",
      "<pre><code>model = VAE(in_dim=10, hidden_dim=32, latent_dim=8)\n",
      "\n",
      "# Create an instance of the optimizer.\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
      "\n",
      "# Train the model for a specified number of epochs.\n",
      "trained_model, loss = train(model, train_data, optimizer, num_epochs=1000)\n",
      "</code></pre>\n",
      "\". Please explain about the 'difficulty class' and 'general ruleset' and 'grenular breakdown' for question\n",
      "Answer:\n",
      "difficulty class : Basic Level Knowledge,general ruleset : B1-Syntax,granular breakdown : 기본 문법/타입 변환\n"
     ]
    }
   ],
   "source": [
    "example_prompt = PromptTemplate.from_template(\n",
    "    \"Question:\\n{question}\\nAnswer:\\n{answer}\"\n",
    ")\n",
    "\n",
    "print(example_prompt.format(**examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question:\\n{question}\\nAnswer:\",\n",
    "    input_variables=[\"question\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, examples=[{'question': 'Here is the question about software development. The title of the question is \"Strange error: TypeError: forward() takes 2 positional arguments but 3 were given\". and body of the question is \"<p>I am trying to train a VAE model. However I keep getting the following error:</p>\\n<pre><code>Traceback (most recent call last):\\n  File &quot;/Users/devcharaf/Documents/Uni/UdeM/Internship/newGNN/app/train.py&quot;, line 28, in &lt;module&gt;\\n    trained_model, loss = train(model, train_data, optimizer, num_epochs=1000, model_type=&quot;VAE&quot;)\\n  File &quot;/Users/devcharaf/Documents/Uni/UdeM/Internship/newGNN/app/utils.py&quot;, line 322, in train\\n    recon_x, mean, logvar = model(data[&quot;x&quot;], data[&quot;edge_index&quot;])\\n  File &quot;/opt/anaconda3/envs/pygeometric/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1194, in _call_impl\\n    return forward_call(*input, **kwargs)\\n  File &quot;/Users/devcharaf/Documents/Uni/UdeM/Internship/newGNN/app/model.py&quot;, line 47, in forward\\n    mean, logvar = self.encoder_forward(x, edge_index)\\n  File &quot;/Users/devcharaf/Documents/Uni/UdeM/Internship/newGNN/app/model.py&quot;, line 53, in encoder_forward\\n    x = self.encoder(x, edge_index)\\n  File &quot;/opt/anaconda3/envs/pygeometric/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1194, in _call_impl\\n    return forward_call(*input, **kwargs)\\nTypeError: forward() takes 2 positional arguments but 3 were given\\n</code></pre>\\n<p>I tried many combinations of arguments but nothing seems to work. Can you spot the error?\\nAs you can see in the following model, the forward function was called in the train() function with only two arguments and not 3. Also the error hints at the line self.encoder(x, edge_index). But when I try to remove one of these arguments, I get an error saying not enough arguments.\\nHere is my model:</p>\\n<pre><code># Variational Auto Encoder\\nclass VAE(nn.Module):\\n    def __init__(self, in_dim, hidden_dim, latent_dim):\\n        super(VAE, self).__init__()\\n        self.in_dim = in_dim\\n        self.hidden_dim = hidden_dim\\n        self.latent_dim = latent_dim\\n\\n        self.encoder = nn.Sequential(\\n            gnn.GCNConv(in_dim, hidden_dim),\\n            nn.ReLU(),\\n            gnn.GCNConv(hidden_dim, hidden_dim),\\n            nn.ReLU()\\n        )\\n        self.fc_mean = nn.Linear(hidden_dim, latent_dim)\\n        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\\n        self.decoder = nn.Sequential(\\n            gnn.GCNConv(latent_dim, hidden_dim),\\n            nn.ReLU(),\\n            gnn.GCNConv(hidden_dim, in_dim),\\n            nn.Sigmoid()\\n        )\\n\\n    def forward(self, x, edge_index):\\n        mean, logvar = self.encoder_forward(x, edge_index)\\n        z = self.reparameterize(mean, logvar)\\n        recon_x = self.decoder_forward(z, edge_index)\\n        return recon_x, mean, logvar\\n\\n    def encoder_forward(self, x, edge_index):\\n        x = self.encoder(x, edge_index)\\n        mean = self.fc_mean(x)\\n        logvar = self.fc_logvar(x)\\n        return mean, logvar\\n\\n    def decoder_forward(self, x, edge_index):\\n        x = self.decoder(x, edge_index)\\n        return x\\n\\n    def reparameterize(self, mean, logvar):\\n        std = torch.exp(0.5 * logvar)\\n        eps = torch.randn_like(std)\\n        return eps.mul(std).add_(mean)\\n</code></pre>\\n<p>and here is my train function:</p>\\n<pre><code>def train(model, data, optimizer, num_epochs):\\n    model.train()\\n    criterion = nn.MSELoss()\\n    beta = 1\\n    epoch_losses = []\\n    for epoch in range(num_epochs):\\n        optimizer.zero_grad()\\n\\n        # Output of the model\\n        recon_x, mean, logvar = model(data[&quot;x&quot;], data[&quot;edge_index&quot;])\\n        output = recon_x\\n        kl_loss = -0.5 * torch.mean(1 + logvar - mean.pow(2) - logvar.exp())\\n        kl_loss *= beta\\n        kl_loss.backward(retain_graph=True)\\n\\n        target = torch.zeros(data[&quot;num_nodes&quot;], 1)\\n\\n        # Loss computation\\n        loss = criterion(output, target)\\n        loss += kl_loss\\n\\n        loss.backward()\\n        optimizer.step()\\n        epoch_losses.append(loss.item())\\n    \\n    return model, epoch_losses\\n</code></pre>\\n<p>Here is the code I am using to train the model:</p>\\n<pre><code>model = VAE(in_dim=10, hidden_dim=32, latent_dim=8)\\n\\n# Create an instance of the optimizer.\\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\\n\\n# Train the model for a specified number of epochs.\\ntrained_model, loss = train(model, train_data, optimizer, num_epochs=1000)\\n</code></pre>\\n\". Please explain about the \\'difficulty class\\' and \\'general ruleset\\' and \\'grenular breakdown\\' for question', 'answer': 'difficulty class : Basic Level Knowledge,general ruleset : B1-Syntax,granular breakdown : 기본 문법/타입 변환'}, {'question': 'Here is the question about software development. The title of the question is \"ValueError: cannot convert float NaN to integer when making wind rose\". and body of the question is \"<p>I wrote a code to create monthly pollution wind roses for a county in California. Pollution roses are similar to wind roses as they show the distribution of wind directions, but instead of showing the magnitude of the wind speed, they plot the concentration of PM2.5. I have used this code for many data sets from the California Air Resources Board, but now I am using data from a local monitoring network and get the following error message when I run my code:</p>\\n<pre><code>Traceback (most recent call last):\\n\\n  File &quot;C:\\\\***.py&quot;, line 341, in __call__\\n    return printer(obj)\\n\\n  File &quot;C:\\\\***.py&quot;, line 253, in &lt;lambda&gt;\\n    png_formatter.for_type(Figure, lambda fig: print_figure(fig, \\'png\\', **kwargs))\\n\\n  File &quot;C:\\\\***.py&quot;, line 137, in print_figure\\n    fig.canvas.print_figure(bytes_io, **kw)\\n\\n  File &quot;C:\\\\***.py&quot;, line 2230, in print_figure\\n    self.figure.draw(renderer)\\n\\n  File &quot;C:\\\\***.py&quot;, line 74, in draw_wrapper\\n    result = draw(artist, renderer, *args, **kwargs)\\n\\n  File &quot;C:\\\\***.py&quot;, line 51, in draw_wrapper\\n    return draw(artist, renderer, *args, **kwargs)\\n\\n  File &quot;C:\\\\***.py&quot;, line 2780, in draw\\n    mimage._draw_list_compositing_images(\\n\\n  File &quot;C:\\\\***.py&quot;, line 132, in _draw_list_compositing_images\\n    a.draw(renderer)\\n\\n  File &quot;C:\\\\***.py&quot;, line 431, in wrapper\\n    return func(*inner_args, **inner_kwargs)\\n\\n  File &quot;C:\\\\***.py&quot;, line 431, in wrapper\\n    return func(*inner_args, **inner_kwargs)\\n\\n  File &quot;C:\\\\***.py&quot;, line 960, in draw\\n    center = self.transWedge.transform((0.5, 0.5))\\n\\n  File &quot;C:\\\\***.py&quot;, line 1765, in transform\\n    return self.transform_affine(values)\\n\\n  File &quot;C:\\\\***.py&quot;, line 1830, in transform_affine\\n    mtx = self.get_matrix()\\n\\n  File &quot;C:\\\\***.py&quot;, line 2619, in get_matrix\\n    inl, inb, inw, inh = self._boxin.bounds\\n\\n  File &quot;C:\\\\***.py&quot;, line 395, in bounds\\n    (x0, y0), (x1, y1) = self.get_points()\\n\\n  File &quot;C:\\\\***.py&quot;, line 759, in get_points\\n    wedge = mpatches.Wedge(self._center, points[1, 1],\\n\\n  File &quot;C:\\\\***.py&quot;, line 1167, in __init__\\n    self._recompute_path()\\n\\n  File &quot;C:\\\\***.py&quot;, line 1179, in _recompute_path\\n    arc = Path.arc(theta1, theta2)\\n\\n  File &quot;C:\\\\***.py&quot;, line 950, in arc\\n    n = int(2 ** np.ceil((eta2 - eta1) / halfpi))\\n\\nValueError: cannot convert float NaN to integer\\n</code></pre>\\n<p><a href=\"https://drive.google.com/file/d/1y3vrPRPPQA7RT6bRlQGzSVqGGY0Fdtq4/view?usp=sharing\" rel=\"nofollow noreferrer\">Here is a link to my csv file</a></p>\\n<p>Here is my code:</p>\\n<pre><code>import pandas as pd\\nfrom windrose import WindroseAxes\\nimport matplotlib.pyplot as plt\\nimport matplotlib.cm as cm\\n\\nwr = pd.read_csv(\\'IVANCALEX_forSO.csv\\')\\nwr = wr.set_index(\\'date\\')\\nwr.index = pd.to_datetime(wr.index)\\n\\nwr[&quot;Month&quot;] = wr.index.month\\nwr[\\'Hour\\'] = wr.index.hour\\n\\nmonth_dict = {1: &quot;January&quot;, 2: &quot;February&quot;, 3: &quot;March&quot;, 4: &quot;April&quot;,\\n               5: &quot;May&quot;, 6: &quot;June&quot;, 7: &quot;July&quot;, 8: &quot;August&quot;, 9: &quot;September&quot;,\\n               10: &quot;October&quot;, 11: &quot;November&quot;, 12: &quot;December&quot;}\\n\\nxval = [&quot;dir_3135&quot;]\\nyval = [\\'Calexico, 604 Kubler Rd\\', \\'Calexico, Alvarez\\', \\'Calexico, Encinas Ave and Ethel St\\', \\'Calexico, Ethel\\',\\n       \\'Calexico, Housing Authority\\', \\'Calexico, Housing Authority West\\', \\'Calexico, Residence\\', \\n       \\'Holtville, 1015 Miller Rd\\', \\'Holtville, South\\', \\'1201 West Hwy 98\\']\\n\\nmonths = [v for k,v in month_dict.items()]\\nnrows, ncols = 2,6\\n\\n#bins=np.logspace(0, 4, num=5) #pm10\\n#bins=np.arange(0, 1, .2) #pm2.5/pm10\\n\\nfor x,y in zip(xval,yval):\\n    fig = plt.figure(figsize=(15, 10))\\n    plt.subplots_adjust(hspace=0.5)\\n    site_name = y.split(&quot;,&quot;)[0].replace(&quot; &quot;, &quot;_&quot;)\\n    fname = f&quot;pollutionrose_{site_name}.png&quot;\\n    bins=[-60,-10,0,10,40] #ozone deviations\\n    fig.tight_layout()\\n    for i, month in enumerate(months):\\n        d =  wr[wr[&quot;Month&quot;].eq(month)].reset_index(drop=True)\\n        ax = fig.add_subplot(nrows, ncols, i + 1, projection=&quot;windrose&quot;)\\n        ax.set_title(month.capitalize(),fontsize=20, weight=\\'bold\\')\\n        ax.bar(d[x], d[y],\\n           normed=True, opening=0.8,\\n           bins=bins, cmap=cm.rainbow,\\n           nsector=8)\\n        ax.set_xticklabels([\\'E\\', \\'N-E\\', \\'N\\', \\'N-W\\', \\'W\\', \\'S-W\\', \\'S\\', \\'S-E\\'],fontsize=18)\\n        ax.tick_params(axis=&quot;y&quot;, labelsize=12.5)\\n        #ax.set_legend(decimal_places=1,fontsize=\\'x-large\\', loc=\\'best\\')\\n        #ax.set_yticklabels(np.arange(11, 77, step=11), fontsize=18)\\n    ax.figure.savefig(fname, dpi=400) #(8, 56, step=8)\\n</code></pre>\\n<p>I am unsure why I am getting this error message because I have worked with data that has many NaN values in the past and had no problem. Are there potentially too many NaN values to perform this analysis?</p>\\n<p>I tried to make this modification:</p>\\n<pre><code>for i, month in enumerate(months):\\n    d =  wr[wr[&quot;Month&quot;].eq(month)].reset_index(drop=True)\\n    ax = fig.add_subplot(nrows, ncols, i + 1, projection=&quot;windrose&quot;)\\n    ax.set_title(month.capitalize(),fontsize=20, weight=\\'bold\\')\\n\\n    # Drop rows with NaN values in d[x] or d[y]\\n    if d[x].isna().any() or d[y].isna().any():\\n        d = d.dropna(subset=[x, y])\\n    \\n    ax.bar(d[x], d[y],\\n           normed=True, opening=0.8,\\n           bins=bins, cmap=cm.rainbow,\\n           nsector=8)\\n    ax.set_xticklabels([\\'E\\', \\'N-E\\', \\'N\\', \\'N-W\\', \\'W\\', \\'S-W\\', \\'S\\', \\'S-E\\'],fontsize=18)\\n    ax.tick_params(axis=&quot;y&quot;, labelsize=12.5)\\n\\n</code></pre>\\n<p>but it did not seem to resolve the issue. Ultimately I want it to look like the picture attached. <a href=\"https://i.stack.imgur.com/xLQRH.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/xLQRH.png\" alt=\"pollution rose\" /></a></p>\\n<p>UPDATE: I tried to replace the NaN values with -250 since I don\\'t need those values anyway and I am still getting the same error message. The error comes from this portion of the code:</p>\\n<pre><code>ax.bar(d[x], d[y],\\n           normed=True, opening=0.8,\\n           bins=bins, cmap=cm.rainbow,\\n           nsector=8)\\n</code></pre>\\n<p>When I look at the d variable, there is no data written so the code is attempting to make a windrose with no data. I am not sure why this is happening.</p>\\n<p>please help!</p>\\n\". Please explain about the \\'difficulty class\\' and \\'general ruleset\\' and \\'grenular breakdown\\' for question', 'answer': 'difficulty class : Intermediate Level ,general ruleset : I2-Performance,granular breakdown : 성능 최적화/자료구조'}, {'question': 'Here is the question about software development. The title of the question is \"What is wrong with my ARIMA model script?\". and body of the question is \"<p>I have to create ARIMA model for U.S GDP forecast. However, script does not work at all and I cannot fix that.</p>\\n<p>I created a code</p>\\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\\nfrom statsmodels.tsa.arima.model import ARIMA\\n\\n\\ndata = {\\n   &quot;Date&quot;: [&quot;01/01/2010&quot;, &quot;01/04/2010&quot;, &quot;01/07/2010&quot;, &quot;01/10/2010&quot; , &quot;01/01/2011&quot; , &quot;01/04/2011&quot; , &quot;01/07/2011&quot; , &quot;01/10/2011&quot; , &quot;01/01/2012&quot; , &quot;01/04/2012&quot; , &quot;01/07/2012&quot; , &quot;01/10/2012&quot; , &quot;01/01/2013&quot; , &quot;01/04/2013&quot; , &quot;01/07/2013&quot; , &quot;01/10/2013&quot; , &quot;01/01/2014&quot; , &quot;01/04/2014&quot; , &quot;01/07/2014&quot; , &quot;01/10/2014&quot; , &quot;01/01/2015&quot; , &quot;01/04/2015&quot; , &quot;01/07/2015&quot; , &quot;01/10/2015&quot; , &quot;01/01/2016&quot; , &quot;01/04/2016&quot; , &quot;01/07/2016&quot; , &quot;01/10/2016&quot; , &quot;01/01/2017&quot; , &quot;01/04/2017&quot; , &quot;01/07/2017&quot; , &quot;01/10/2017&quot; , &quot;01/01/2018&quot; , &quot;01/04/2018&quot; , &quot;01/07/2018&quot; , &quot;01/10/2018&quot; , &quot;01/01/2019&quot; , &quot;01/04/2019&quot; , &quot;01/07/2019&quot; , &quot;01/10/2019&quot; , &quot;01/01/2020&quot; , &quot;01/04/2020&quot; , &quot;01/07/2020&quot; , &quot;01/10/2020&quot; ],\\n   &quot;GDP(valid)&quot;: [14.764, 14.98, 15.141, 15.309, 15.351, 15.557, 15.647, 15.842, 16.068, 16.207, 16.319, 16.42, 16.648, 16.728, 16.953, 17.192, 17.197, 17.518, 17.804, 17.912, 18.063, 18.279, 18.401, 18.435, 18.525, 18.711, 18.892, 19.089, 19.28, 19.438, 19.692, 20.037, 30.328, 20.58, 20.798, 20.917, 21.104, 21.384, 21.694, 22.092, 21.706, 19.913, 21.647, 22.024 ],\\n   &quot;Unemployment&quot;: [4.85, 4.84, 4.836, 4.828, 4.82, 4.81, 4.8, 4.79, 4.78, 4.77, 4.76, 4.75, 4.74, 4.73, 4.72, 4.718, 4.71, 4.7, 4.696, 4.69, 4.683, 4.677, 4.67, 4.67, 4.657, 4.65, 4.64, 4.635, 4.628, 4.62, 4.61, 4.65, 4.6, 4.588, 4.58, 4.57, 4.56, 4.548, 4.536, 4.524, 4.51, 4.5, 4.49, 4.48 ],\\n   &quot;Interest rate&quot;: [3.71, 3.49, 2.78, 2.86, 3.46, 3.21, 2.42, 2.04, 2.03, 1.82, 1.64, 1.7, 1.95, 2, 2.71, 2.74, 2.76, 2.62, 2.5, 2.28, 2, 2.16, 2.22, 2.19, 1.92, 1.75, 1.56, 2.13, 2.44, 2.26, 2.24, 2.37, 2.76, 2.92, 2.92, 3.03, 2.65, 2.33, 1.8, 1.79, 1.37, 0.68, 0.65, 0.86 ],\\n   &quot;Import volume&quot;: [563.166, 588.05, 604.7, 619.5, 652.4, 674.9, 682.4, 688.3, 698.7, 696.6, 688.1, 670, 688, 688, 670, 694, 710, 723.3, 720.1, 722.4, 700, 697.3, 694.3, 680.06, 666.2, 674.9, 686.2, 693, 712.6, 730.35, 724.18, 754.28, 770.353, 773.616, 785.58, 791.5, 782.6, 785.9, 776.96, 760.26, 733.363, 612.47, 709.62, 757.57 ],\\n   &quot;Export volume&quot;: [1759.9, 1819.7, 1877.2, 1972.1, 2040.4, 2116.1, 2156.8, 2150.1, 2190, 2215.6, 2227.7, 2237.5, 2255.9, 2261.4, 2285.1, 2349.2, 2343.6, 2395.2, 2399.2, 2376.2, 2299, 2300.4, 2260.7, 2222.3, 2182, 2222, 2270, 2268.4, 2339.7, 2349.7, 2384.5, 2479.1, 2517.3, 2562.2, 2535.5, 2537.4, 2544.1, 2545.4, 2532.4, 2531.9, 2416.1, 1811.3, 2106.6, 2266.4 ],\\n   &quot;State. Expenditure&quot;: [3710.5, 3745.7, 3792.44, 3809.8, 3811.3, 3849.4, 3783.8, 3786.5, 3774.3, 3772.8, 3777.3, 3769.5, 3758.8, 3776, 3773.7, 3772.4, 3819.4, 3876, 3931.6, 3926.6, 3970.2, 3990.8, 4011.9, 4050.4, 4084.3, 4101.6, 4150.5, 4175.5, 4119.2, 4195.5, 4254, 4320.4, 4398.5, 4453.8, 4512.2, 4593.4, 4688.5, 4736.9, 4776.8, 4792, 4870.1, 8830, 7114.8, 5863.6 ],\\n   &quot;Inflation&quot;: [2.6, 2.2, 1.2, 1.2, 1.6, 3.2, 3.6, 3.5, 2.9, 2.3, 1.4, 2.2, 1.6, 1.1, 2, 1, 1.6, 2, 2, 1.7, -0.1, -0.2, 0.2, 0.2, 1.4, 1.1, 0.8, 1.6, 2.5, 2.2, 1.7, 2, 2.1, 2.5, 2.9, 2.5, 1.6, 2, 1.8, 2.5, 0.3, 1, 1.2 ]\\n}\\n\\n\\n# Convert Data to Time Series Format\\ndf = pd.DataFrame(data)\\ndf[&quot;Date&quot;] = pd.to_datetime(df[&quot;Date&quot;])\\ndf.set_index(&quot;Date&quot;, inplace=True)\\n\\n\\n# Construction of ARIMA model with lags for all variables\\np, d, q = 1, 1, 1  # ARIMA model parameters (can be changed if necessary)\\nmodel = ARIMA(df[&quot;GDP(valid)&quot;], order=(p, d, q), exog=df[[&quot;Unemployment&quot;, &quot;Interest rate&quot;, &quot;Import volume&quot;, &quot;Export volume&quot;, &quot;State. Expenditure&quot;, &quot;Inflation&quot;]])\\n\\n\\n# ARIMA model fit to data\\nmodel_fit = model.fit()\\n\\n\\n# Forecast for the 1st quarter of 2021\\nforecast_start = \\'2021-01-01\\'\\nforecast_end = \\'2021-04-01\\'\\nexog_forecast = df.loc[forecast_start:forecast_end, [&quot;Unemployment&quot;, &quot;Interest rate&quot;, &quot;Import volume&quot;, &quot;Export volume&quot;, &quot;State. Expenditure&quot;, &quot;Inflation&quot;]]\\nforecast = model_fit.get_forecast(steps=len(exog_forecast), exog=exog_forecast)\\n\\n\\n# Forecast output\\nforecast_index = pd.date_range(start=forecast_start, end=forecast_end, freq=\\'QS\\')\\nforecast_df = pd.DataFrame(forecast.predicted_mean.values, index=forecast_index, columns=[&quot;GDP Forecast&quot;])\\nprint(forecast_df)\\n</code></pre>\\n<p>And result are always the same:</p>\\n<pre class=\"lang-py prettyprint-override\"><code>/Users/yermadimukashov/PycharmProjects/pythonProject/venv/bin/python /Users/yermadimukashov/PycharmProjects/pythonProject/main.py \\nTraceback (most recent call last):\\n  File &quot;/Users/yermadimukashov/PycharmProjects/pythonProject/main.py&quot;, line 16, in &lt;module&gt;\\n    df = pd.DataFrame(data)\\n  File &quot;/Users/yermadimukashov/PycharmProjects/pythonProject/venv/lib/python3.9/site-packages/pandas/core/frame.py&quot;, line 733, in __init__\\n    mgr = dict_to_mgr(data, index, columns, dtype=dtype, copy=copy, typ=manager)\\n  File &quot;/Users/yermadimukashov/PycharmProjects/pythonProject/venv/lib/python3.9/site-packages/pandas/core/internals/construction.py&quot;, line 503, in dict_to_mgr\\n    return arrays_to_mgr(arrays, columns, index, dtype=dtype, typ=typ, consolidate=copy)\\n  File &quot;/Users/yermadimukashov/PycharmProjects/pythonProject/venv/lib/python3.9/site-packages/pandas/core/internals/construction.py&quot;, line 114, in arrays_to_mgr\\n    index = _extract_index(arrays)\\n  File &quot;/Users/yermadimukashov/PycharmProjects/pythonProject/venv/lib/python3.9/site-packages/pandas/core/internals/construction.py&quot;, line 677, in _extract_index\\n    raise ValueError(&quot;All arrays must be of the same length&quot;)\\nValueError: All arrays must be of the same length\\n\\nProcess finished with exit code 1\\n</code></pre>\\n\". Please explain about the \\'difficulty class\\' and \\'general ruleset\\' and \\'grenular breakdown\\' for question', 'answer': 'difficulty class : Basic Level Knowledge,general ruleset : B1-Syntax,granular breakdown : 기본 문법/타입 변환'}], example_prompt=PromptTemplate(input_variables=['answer', 'question'], input_types={}, partial_variables={}, template='Question:\\n{question}\\nAnswer:\\n{answer}'), suffix='Question:\\n{question}\\nAnswer:')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44, 2)\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(first_ann_q_id[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.iloc[first_ann_q_id[3], 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here is the question about software development. The title of the question is \"Add error handling in Python API call for Meraki\". and body of the question is \"<p>I have this script to make an API call using the meraki python module to query a Meraki device.</p>\n",
      "<p>I'm trying to add some error handling in the code so that if the API call comes back with an error code, it will do something else. I cannot seem to figure out what to do.</p>\n",
      "<p>Here is my simple code to just query a device:</p>\n",
      "<pre><code>import meraki\n",
      "import requests\n",
      "\n",
      "API_KEY = 'API_KEY'\n",
      "dashboard = meraki.DashboardAPI(API_KEY)\n",
      "\n",
      "serial = input(&quot;What is the serial number?&quot;)\n",
      "print(f&quot;{serial}&quot;)\n",
      "\n",
      "response = (dashboard.devices.getDevice(serial))\n",
      "</code></pre>\n",
      "<p>When I run the script it will return either a &quot;200 OK&quot; or &quot;404 Not Found&quot;</p>\n",
      "<p>Terminal window response:</p>\n",
      "<pre class=\"lang-none prettyprint-override\"><code>C:\\Scripts\\Meraki\\dev&gt; python .\\getdevice.py\n",
      "What is the serial number? XXXX-XXXX-XXXX\n",
      "\n",
      "2023-04-26 18:32:52       meraki:     INFO &gt; GET https://api.meraki.com/api/v1/devices/XXXX-XXXX-XXXX\n",
      "2023-04-26 18:32:53       meraki:     INFO &gt; devices, getDevice - 200 OK\n",
      "</code></pre>\n",
      "<p>or</p>\n",
      "<pre class=\"lang-none prettyprint-override\"><code>2023-04-26 18:41:09       meraki:     INFO &gt; GET https://api.meraki.com/api/v1/devices/XXXX-XXXX-XXXX\n",
      "2023-04-26 18:41:10       meraki:    ERROR &gt; devices, getDevice - 404 Not Found, b''\n",
      "</code></pre>\n",
      "<p>The purpose of this script is to check to see if the serial number has already been assigned to a user / network. If it is available, I'll get a &quot;404 Not Found&quot; and if it has already been assigned to someone, I'll get a &quot;404 Not Found&quot;.</p>\n",
      "<p>How can I detect and handle the 404 case?</p>\n",
      "\". Please explain about the 'difficulty class' and 'general ruleset' and 'grenular breakdown' for question\n"
     ]
    }
   ],
   "source": [
    "prompt = \" {question}\"  # Example prompt\n",
    "question = df.iloc[first_ann_q_id[3], 0]\n",
    "\n",
    "# Format the prompt with the question\n",
    "try:\n",
    "    final_prompt = prompt.format(question=question)\n",
    "    print(final_prompt)\n",
    "except KeyError as e:\n",
    "    print(f\"Missing placeholder in prompt: {e}\")\n",
    "except IndexError as e:\n",
    "    print(f\"Index error in prompt formatting: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided flowchart, I'll analyze the difficulty level of the question.\n",
      "\n",
      "The question is about adding error handling in a Python API call. Following the decision path:\n",
      "\n",
      "1. Is it about basic syntax/environment? No\n",
      "2. Is it about performance/optimization? No\n",
      "3. Is it about frameworks? Yes (Meraki API)\n",
      "\n",
      "So, the category code is I3 (Intermediate Level Knowledge).\n",
      "\n",
      "The required knowledge for this question includes:\n",
      "\n",
      "* Web frameworks (K6)\n",
      "* Error handling ( implicit in K3, but not explicitly listed; however, error handling is a fundamental concept that can be applied at various levels)\n",
      "\n",
      "Therefore, the difficulty level and required knowledge for this question are:\n",
      "\n",
      "[Intermediate Level Knowledge] | [I3] | [Web frameworks/Error handling]\n",
      "\n",
      "**Difficulty Class:** Intermediate\n",
      "\n",
      "The question requires understanding of web frameworks (Meraki API) and error handling concepts. The solution involves applying these concepts to a specific problem, which is characteristic of intermediate-level questions.\n",
      "\n",
      "**General Ruleset:**\n",
      "\n",
      "* The question assumes basic knowledge of Python syntax and data types.\n",
      "* It requires understanding of web frameworks and how to interact with APIs.\n",
      "* Error handling is an essential concept in this context.\n",
      "\n",
      "**Granular Breakdown:**\n",
      "\n",
      "* Understanding the Meraki API and its usage (K6)\n",
      "* Familiarity with error handling concepts ( implicit in K3)\n",
      "* Ability to apply these concepts to a specific problem\n",
      "\n",
      "Overall, this question requires intermediate-level knowledge of Python programming, specifically in the areas of web frameworks and error handling."
     ]
    }
   ],
   "source": [
    "# 질의\n",
    "answer = llm.stream(question)\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_stackoverflow_src",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
