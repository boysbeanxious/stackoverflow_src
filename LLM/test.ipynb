{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from statistics import mode\n",
    "# from ollama import chat\n",
    "# # from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "# def gen_answer():\n",
    "\n",
    "#     response = chat(\n",
    "#             model='llama-3.1-70b-instruct-lorablated.Q4_K_M:latest',\n",
    "#             messages=[\n",
    "#                 # {\"role\": \"system\", \"content\": sys_promt},\n",
    "#                 {'role': 'user','content': \n",
    "#                     \"John found that the average of 15 numbers is 40.\"\n",
    "#                     \"If 10 is added to each number then the mean of the numbers is?\"\n",
    "#                     \"Report the answer surrounded by backticks (example: `123`)\"}\n",
    "                \n",
    "#             ],\n",
    "#     )   \n",
    "#     match = re.search(r'`(\\d+)`', response)\n",
    "#     if match is None:\n",
    "#         return None\n",
    "#     return match.group(1)\n",
    "\n",
    "# answers = [gen_answer() for i in range(5)]\n",
    "\n",
    "# print(\n",
    "#     f\"Answers: {answers}\\n\",\n",
    "#     f\"Final answer: {mode(answers)}\",\n",
    "#     )\n",
    "\n",
    "# # Sample runs of Llama-3-70B (all correct):\n",
    "# # ['60', '50', '50', '50', '50'] -> 50\n",
    "# # ['50', '50', '50', '60', '50'] -> 50\n",
    "# # ['50', '50', '60', '50', '50'] -> 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "p = os.path.abspath('..')\n",
    "# p = p+r'\\config'\n",
    "sys.path.insert(1, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.annotation.Self_Consistency as sc\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multiprocessing import Process\n",
    "    \n",
    "# def task(llm_model, few_shot_n, test_n, q_src_yn, ver, p_ver):\n",
    "\n",
    "#     print(f\"Task {llm_model}_{few_shot_n}_{test_n}_{q_src_yn}_{p_ver} 시작\")\n",
    "#     for i in range(ver):\n",
    "#         print(f\"Task {llm_model}_{few_shot_n}_{test_n}_{q_src_yn}_{p_ver} 실행 중: {i}\")\n",
    "#         sc.Self_Consistency(  llm_model\n",
    "#                         , few_shot_n\n",
    "#                         , test_n\n",
    "#                         , q_src_yn\n",
    "#                         , i\n",
    "#                         , p_ver)  \n",
    "    \n",
    "#     print(f\"Task {llm_model}_{few_shot_n}_{test_n}_{q_src_yn}_{p_ver} 완료\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     process1 = Process(target=task, args=('l', 1, 10, 'Y', 10, 'sys_prompt3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.Self_Consistency( 'l'\n",
    "#                         , 3\n",
    "#                         , 10\n",
    "#                         , 'Y'\n",
    "#                         , 10\n",
    "#                         , 'sys_prompt4'\n",
    "#                         ,5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multiprocessing import Process\n",
    "    \n",
    "# def task(llm_model, few_shot_n, test_n, q_src_yn, ver, p_ver, sc_num):\n",
    "\n",
    "#     print(f\"Task {llm_model}_{few_shot_n}_{test_n}_{q_src_yn}_{p_ver}_{sc_num} 시작\")\n",
    "#     for i in range(ver):\n",
    "#         print(f\"Task {llm_model}_{few_shot_n}_{test_n}_{q_src_yn}_{p_ver}_{sc_num} 실행 중: {i}\")\n",
    "#         sc.Self_Consistency(  llm_model\n",
    "#                         , few_shot_n\n",
    "#                         , test_n\n",
    "#                         , q_src_yn\n",
    "#                         , i\n",
    "#                         , p_ver\n",
    "#                         , sc_num)  \n",
    "    \n",
    "#     print(f\"Task {llm_model}_{few_shot_n}_{test_n}_{q_src_yn}_{p_ver}_{sc_num} 완료\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     process1 = Process(target=task, args=('l', 1, 5, 'Y', 10, 'sys_prompt4', 5))\n",
    "#     process2 = Process(target=task, args=('l', 1, 5, 'N', 10, 'sys_prompt4', 5))\n",
    "\n",
    "#     process3 = Process(target=task, args=('l', 2, 5, 'Y', 10, 'sys_prompt4', 5))\n",
    "#     process4 = Process(target=task, args=('l', 2, 5, 'N', 10, 'sys_prompt4', 5))\n",
    "\n",
    "#     process5 = Process(target=task, args=('l', 3, 5, 'Y', 10, 'sys_prompt4', 5))\n",
    "#     process6 = Process(target=task, args=('l', 3, 5, 'N', 10, 'sys_prompt4', 5))\n",
    "    \n",
    "#     process1.start()\n",
    "#     process2.start()\n",
    "#     process3.start()\n",
    "#     process4.start()\n",
    "#     process5.start()\n",
    "#     process6.start()\n",
    "    \n",
    "#     process1.join()\n",
    "#     process2.join()\n",
    "#     process3.join()\n",
    "#     process4.join()\n",
    "#     process5.join()\n",
    "#     process6.join()\n",
    "#     print(\"모든 작업 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task l_1_5_Y_sys_prompt7_5 시작\n",
      "Task l_1_5_N_sys_prompt7_5 시작Task l_1_5_Y_sys_prompt7_5 실행 중: 0\n",
      "\n",
      "Task l_1_5_N_sys_prompt7_5 실행 중: 0Task l_2_5_Y_sys_prompt7_5 시작\n",
      "\n",
      "Task l_2_5_Y_sys_prompt7_5 실행 중: 0Task l_2_5_N_sys_prompt7_5 시작\n",
      "\n",
      "Task l_2_5_N_sys_prompt7_5 실행 중: 0Task l_3_5_Y_sys_prompt7_5 시작\n",
      "\n",
      "Task l_3_5_Y_sys_prompt7_5 실행 중: 0\n",
      "Task l_3_5_N_sys_prompt7_5 시작\n",
      "Task l_3_5_N_sys_prompt7_5 실행 중: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [45:42, 304.68s/it]]\n",
      "Process Process-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_392283/3113568174.py\", line 8, in task\n",
      "    sc.Self_Consistency(  llm_model\n",
      "  File \"/home/mghan/sopjt/git/stackoverflow_src/lib/annotation/Self_Consistency.py\", line 24, in __init__\n",
      "    self.calc_acc(llm_model, few_shot_n, q_src_yn)\n",
      "  File \"/home/mghan/sopjt/git/stackoverflow_src/lib/annotation/Self_Consistency.py\", line 136, in calc_acc\n",
      "    self.calc_acc_for_l(llm_model, few_shot_n, q_src_yn)\n",
      "  File \"/home/mghan/sopjt/git/stackoverflow_src/lib/annotation/Self_Consistency.py\", line 104, in calc_acc_for_l\n",
      "    response = chat( model      = self.ollama,\n",
      "  File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/ollama/_client.py\", line 236, in chat\n",
      "    return self._request_stream(\n",
      "  File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/ollama/_client.py\", line 99, in _request_stream\n",
      "    return self._stream(*args, **kwargs) if stream else self._request(*args, **kwargs).json()\n",
      "  File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/ollama/_client.py\", line 75, in _request\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: health resp: Get \"http://127.0.0.1:43199/health\": read tcp 127.0.0.1:43456->127.0.0.1:43199: read: connection reset by peer\n",
      "9it [45:42, 304.69s/it]]\n",
      "\n",
      "\n",
      "\n",
      "Process Process-1:\n",
      "Process Process-5:\n",
      "Process Process-6:\n",
      "Process Process-3:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_392283/3113568174.py\", line 8, in task\n",
      "    sc.Self_Consistency(  llm_model\n",
      "  File \"/tmp/ipykernel_392283/3113568174.py\", line 8, in task\n",
      "    sc.Self_Consistency(  llm_model\n",
      "  File \"/tmp/ipykernel_392283/3113568174.py\", line 8, in task\n",
      "    sc.Self_Consistency(  llm_model\n",
      "  File \"/tmp/ipykernel_392283/3113568174.py\", line 8, in task\n",
      "    sc.Self_Consistency(  llm_model\n",
      "  File \"/home/mghan/sopjt/git/stackoverflow_src/lib/annotation/Self_Consistency.py\", line 24, in __init__\n",
      "    self.calc_acc(llm_model, few_shot_n, q_src_yn)\n",
      "  File \"/home/mghan/sopjt/git/stackoverflow_src/lib/annotation/Self_Consistency.py\", line 24, in __init__\n",
      "    self.calc_acc(llm_model, few_shot_n, q_src_yn)\n",
      "  File \"/home/mghan/sopjt/git/stackoverflow_src/lib/annotation/Self_Consistency.py\", line 24, in __init__\n",
      "    self.calc_acc(llm_model, few_shot_n, q_src_yn)\n",
      "  File \"/home/mghan/sopjt/git/stackoverflow_src/lib/annotation/Self_Consistency.py\", line 24, in __init__\n",
      "    self.calc_acc(llm_model, few_shot_n, q_src_yn)\n",
      "  File \"/home/mghan/sopjt/git/stackoverflow_src/lib/annotation/Self_Consistency.py\", line 136, in calc_acc\n",
      "    self.calc_acc_for_l(llm_model, few_shot_n, q_src_yn)\n",
      "  File \"/home/mghan/sopjt/git/stackoverflow_src/lib/annotation/Self_Consistency.py\", line 136, in calc_acc\n",
      "    self.calc_acc_for_l(llm_model, few_shot_n, q_src_yn)\n",
      "  File \"/home/mghan/sopjt/git/stackoverflow_src/lib/annotation/Self_Consistency.py\", line 136, in calc_acc\n",
      "    self.calc_acc_for_l(llm_model, few_shot_n, q_src_yn)\n",
      "  File \"/home/mghan/sopjt/git/stackoverflow_src/lib/annotation/Self_Consistency.py\", line 136, in calc_acc\n",
      "    self.calc_acc_for_l(llm_model, few_shot_n, q_src_yn)\n",
      "  File \"/home/mghan/sopjt/git/stackoverflow_src/lib/annotation/Self_Consistency.py\", line 104, in calc_acc_for_l\n",
      "    response = chat( model      = self.ollama,\n",
      "  File \"/home/mghan/sopjt/git/stackoverflow_src/lib/annotation/Self_Consistency.py\", line 104, in calc_acc_for_l\n",
      "    response = chat( model      = self.ollama,\n",
      "  File \"/home/mghan/sopjt/git/stackoverflow_src/lib/annotation/Self_Consistency.py\", line 104, in calc_acc_for_l\n",
      "    response = chat( model      = self.ollama,\n",
      "  File \"/home/mghan/sopjt/git/stackoverflow_src/lib/annotation/Self_Consistency.py\", line 104, in calc_acc_for_l\n",
      "    response = chat( model      = self.ollama,\n",
      "  File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/ollama/_client.py\", line 236, in chat\n",
      "    return self._request_stream(\n",
      "  File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/ollama/_client.py\", line 236, in chat\n",
      "    return self._request_stream(\n",
      "  File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/ollama/_client.py\", line 236, in chat\n",
      "    return self._request_stream(\n",
      "  File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/ollama/_client.py\", line 99, in _request_stream\n",
      "    return self._stream(*args, **kwargs) if stream else self._request(*args, **kwargs).json()\n",
      "  File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/ollama/_client.py\", line 99, in _request_stream\n",
      "    return self._stream(*args, **kwargs) if stream else self._request(*args, **kwargs).json()\n",
      "  File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/ollama/_client.py\", line 236, in chat\n",
      "    return self._request_stream(\n",
      "  File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/ollama/_client.py\", line 99, in _request_stream\n",
      "    return self._stream(*args, **kwargs) if stream else self._request(*args, **kwargs).json()\n",
      "  File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/ollama/_client.py\", line 75, in _request\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "  File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/ollama/_client.py\", line 75, in _request\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "  File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/ollama/_client.py\", line 99, in _request_stream\n",
      "    return self._stream(*args, **kwargs) if stream else self._request(*args, **kwargs).json()\n",
      "  File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/ollama/_client.py\", line 75, in _request\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: llama runner process no longer running: -1 CUDA error: out of memory\n",
      "  current device: 3, in function alloc at /go/src/github.com/ollama/ollama/llm/llama.cpp/ggml/src/ggml-cuda.cu:376\n",
      "  cuMemCreate(&handle, reserve_size, &prop, 0)\n",
      "/go/src/github.com/ollama/ollama/llm/llama.cpp/ggml/src/ggml-cuda.cu:102: CUDA error\n",
      "ollama._types.ResponseError: llama runner process no longer running: -1 CUDA error: out of memory\n",
      "  current device: 3, in function alloc at /go/src/github.com/ollama/ollama/llm/llama.cpp/ggml/src/ggml-cuda.cu:376\n",
      "  cuMemCreate(&handle, reserve_size, &prop, 0)\n",
      "/go/src/github.com/ollama/ollama/llm/llama.cpp/ggml/src/ggml-cuda.cu:102: CUDA error\n",
      "ollama._types.ResponseError: llama runner process no longer running: -1 CUDA error: out of memory\n",
      "  current device: 3, in function alloc at /go/src/github.com/ollama/ollama/llm/llama.cpp/ggml/src/ggml-cuda.cu:376\n",
      "  cuMemCreate(&handle, reserve_size, &prop, 0)\n",
      "/go/src/github.com/ollama/ollama/llm/llama.cpp/ggml/src/ggml-cuda.cu:102: CUDA error\n",
      "  File \"/home/mghan/sopjt/git/stackoverflow_src/venv_stackoverflow_src/lib/python3.10/site-packages/ollama/_client.py\", line 75, in _request\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: llama runner process no longer running: -1 CUDA error: out of memory\n",
      "  current device: 3, in function alloc at /go/src/github.com/ollama/ollama/llm/llama.cpp/ggml/src/ggml-cuda.cu:376\n",
      "  cuMemCreate(&handle, reserve_size, &prop, 0)\n",
      "/go/src/github.com/ollama/ollama/llm/llama.cpp/ggml/src/ggml-cuda.cu:102: CUDA error\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process\n",
    "    \n",
    "def task(llm_model, few_shot_n, test_n, q_src_yn, ver, p_ver, sc_num):\n",
    "\n",
    "    print(f\"Task {llm_model}_{few_shot_n}_{test_n}_{q_src_yn}_{p_ver}_{sc_num} 시작\")\n",
    "    for i in range(ver):\n",
    "        print(f\"Task {llm_model}_{few_shot_n}_{test_n}_{q_src_yn}_{p_ver}_{sc_num} 실행 중: {i}\")\n",
    "        sc.Self_Consistency(  llm_model\n",
    "                        , few_shot_n\n",
    "                        , test_n\n",
    "                        , q_src_yn\n",
    "                        , i\n",
    "                        , p_ver\n",
    "                        , sc_num)  \n",
    "    \n",
    "    print(f\"Task {llm_model}_{few_shot_n}_{test_n}_{q_src_yn}_{p_ver}_{sc_num} 완료\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process1 = Process(target=task, args=('l', 1, 5, 'Y', 10, 'sys_prompt7', 5))\n",
    "    process2 = Process(target=task, args=('l', 1, 5, 'N', 10, 'sys_prompt7', 5))\n",
    "\n",
    "    process3 = Process(target=task, args=('l', 2, 5, 'Y', 10, 'sys_prompt7', 5))\n",
    "    process4 = Process(target=task, args=('l', 2, 5, 'N', 10, 'sys_prompt7', 5))\n",
    "\n",
    "    process5 = Process(target=task, args=('l', 3, 5, 'Y', 10, 'sys_prompt7', 5))\n",
    "    process6 = Process(target=task, args=('l', 3, 5, 'N', 10, 'sys_prompt7', 5))\n",
    "    \n",
    "    process1.start()\n",
    "    process2.start()\n",
    "    process3.start()\n",
    "    process4.start()\n",
    "    process5.start()\n",
    "    process6.start()\n",
    "    \n",
    "    process1.join()\n",
    "    process2.join()\n",
    "    process3.join()\n",
    "    process4.join()\n",
    "    process5.join()\n",
    "    process6.join()\n",
    "    print(\"모든 작업 완료\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_stackoverflow_src",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
