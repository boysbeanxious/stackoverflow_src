{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "p = os.path.abspath('..')\n",
    "# p = p+r'\\config'\n",
    "sys.path.insert(1, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './result'\n",
    "file_list = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sc_calc_acc_condition_with_temp_with_sc(llm_model, few_shot_n, test_n, q_src_yn, ver, p_ver, sc_num, temp, excel_ver):\n",
    "    tmp = pd.DataFrame()\n",
    "    df_eval = pd.DataFrame()\n",
    "    acc_list = []\n",
    "    path = './result'\n",
    "    # ./result/sc_l_result_4_15_Y_30_sys_prompt8_0.01_ver1_0.csv\n",
    "    file_list = os.listdir(path)\n",
    "    opt_file = [x for x in file_list if x.startswith(f'sc_{llm_model}_result_{few_shot_n}_{test_n}_{q_src_yn}_{ver}_{p_ver}_{sc_num}_{temp}_{excel_ver}')]\n",
    "    opt_file = [x for x in opt_file if x.endswith(f'.csv')]\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    \n",
    "    if len(opt_file)>0 : \n",
    "        for f in opt_file:\n",
    "            tmp = pd.read_csv(f'{path}/{f}', index_col =0)\n",
    "            tmp = tmp.dropna()\n",
    "\n",
    "            tmp['gold'] = tmp['answer_encode'].apply(lambda x : re.sub(r'[^012]', '', x))\n",
    "            tmp['o_result'] = tmp['result'].apply(lambda x : re.sub(r'[^012]', '', x))\n",
    "            tmp = tmp[tmp['o_result'].isin(['1', '0', '2'])]\n",
    "\n",
    "            \n",
    "            gold_df = tmp[['id', 'gold']].drop_duplicates()\n",
    "            chk_cnt = tmp.groupby(['id', 'o_result']).count().reset_index()[['id', 'o_result', 'question']]\n",
    "            chk_cnt = chk_cnt.rename(columns = {'question': 'cnt'})\n",
    "            chk_cnt = chk_cnt[chk_cnt['cnt'] == sc_num]\n",
    "            chk_cnt = chk_cnt.sort_values(by = ['id', 'cnt'], ascending=[True, False]).groupby(['id']).head(1)\n",
    "            df_eval = pd.merge(gold_df, chk_cnt, on = ['id'])\n",
    "\n",
    "            df_eval['equal_yn'] = np.where(df_eval['gold']==df_eval['o_result'], 1, 0)\n",
    "            acc = (df_eval['equal_yn'].sum()/df_eval.shape[0])*100  \n",
    "            acc_list.append(acc)\n",
    "            df = pd.concat([df, df_eval], axis =0)\n",
    "            \n",
    "        df['equal_yn'] = np.where(df['gold']==df['o_result'], 1, 0)\n",
    "        y_true = df['o_result']\n",
    "        y_pred = df['gold']\n",
    "        print(metrics.classification_report(y_true, y_pred, digits=3))\n",
    "        \n",
    "        acc = (df['equal_yn'].sum()/df.shape[0])*100            \n",
    "        print(f'{llm_model}_result_{few_shot_n}_{test_n}_{q_src_yn} : ', acc)\n",
    "        return acc_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process(target=task, args=('l', 1, 10, 'Y', 10, 'sys_prompt10', 3, 0.01, 'ver5'))\n",
    "# Process(target=task, args=('l', 2, 10, 'Y', 10, 'sys_prompt10', 3, 0.01, 'ver5'))\n",
    "# Process(target=task, args=('l', 4, 10, 'Y', 10, 'sys_prompt10', 5, 0.01, 'ver5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.885     0.767     0.821        30\n",
      "           1      0.795     0.912     0.849        34\n",
      "           2      1.000     0.800     0.889         5\n",
      "\n",
      "    accuracy                          0.841        69\n",
      "   macro avg      0.893     0.826     0.853        69\n",
      "weighted avg      0.849     0.841     0.840        69\n",
      "\n",
      "l_result_1_10_Y :  84.05797101449275\n",
      "[np.float64(71.42857142857143), np.float64(50.0), np.float64(100.0), np.float64(88.88888888888889), np.float64(60.0), np.float64(100.0), np.float64(100.0), np.float64(88.88888888888889), np.float64(80.0), np.float64(87.5)]\n"
     ]
    }
   ],
   "source": [
    "list_ =         sc_calc_acc_condition_with_temp_with_sc('l', 1, 10, 'Y', 10, 'sys_prompt10', 3,  0.01, 'ver5')\n",
    "print(list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.935     0.806     0.866        36\n",
      "           1      0.769     0.938     0.845        32\n",
      "           2      1.000     0.714     0.833         7\n",
      "\n",
      "    accuracy                          0.853        75\n",
      "   macro avg      0.902     0.819     0.848        75\n",
      "weighted avg      0.871     0.853     0.854        75\n",
      "\n",
      "l_result_2_10_Y :  85.33333333333334\n",
      "[np.float64(88.88888888888889), np.float64(100.0), np.float64(75.0), np.float64(85.71428571428571), np.float64(62.5), np.float64(100.0), np.float64(75.0), np.float64(100.0), np.float64(100.0), np.float64(71.42857142857143)]\n"
     ]
    }
   ],
   "source": [
    "list_ =         sc_calc_acc_condition_with_temp_with_sc('l', 2, 10, 'Y', 10, 'sys_prompt10', 3,  0.01, 'ver5')\n",
    "print(list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.963     0.929     0.945        28\n",
      "           1      0.867     0.963     0.912        27\n",
      "           2      1.000     0.667     0.800         6\n",
      "\n",
      "    accuracy                          0.918        61\n",
      "   macro avg      0.943     0.853     0.886        61\n",
      "weighted avg      0.924     0.918     0.916        61\n",
      "\n",
      "l_result_4_10_Y :  91.80327868852459\n",
      "[np.float64(100.0), np.float64(100.0), np.float64(87.5), np.float64(85.71428571428571), np.float64(100.0), np.float64(100.0), np.float64(90.0), np.float64(80.0), np.float64(100.0), np.float64(66.66666666666666)]\n"
     ]
    }
   ],
   "source": [
    "list_ =         sc_calc_acc_condition_with_temp_with_sc('l', 4, 10, 'Y', 10, 'sys_prompt10', 5,  0.01, 'ver5')\n",
    "print(list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_stackoverflow_src",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
